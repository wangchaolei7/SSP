# Folder where the checkpoint and results will be saved
save_dir: /home/wangcl/project/SSP/kitti360/video
# Weights (of the same architecture) to load before training, leave null if no pre-training. Checkpoint should be in save_dir
pretrained_checkpoint: null
# Resume the training of a checkpoint in save_dir. The rest of this config will be discarded.
resume_training: null

########################################################################################################################################
# Config used to initalise the dataset
data_cfg:
  # Identifying name of the dataset, see data.datasets_utils.parse_datasets
  dataset: kitti360
  # Root folder containing train/val splits
  path: /home/wangcl/data/open_video_DGSS/kitti360_sequence
  # Root folders for images and labels (use GT as source of truth)
  root_images: /home/wangcl/data/open_video_DGSS/kitti360_sequence/train/data_2d_raw
  root_labels: /home/wangcl/data/open_video_DGSS/kitti360_sequence/train/data_2d_semantics
  # Subfolders used to find RGB frames and labels
  frame_folder: data_rect
  mask_folder: 15semantic
  # Label naming rule: <stem> + label_extension
  img_extension: .png
  label_extension: .png
  # Dataset classes
  num_classes: 15
  # Strict pairing: error on missing pairs (true) or warn + skip (false)
  strict_pairs: false
  # Index of the adjacent frames, relative to the sample frame, which will be loaded alongside it.
  adjacent_frames:
  - -1
  # Input size, as [Height, Width]
  crop_size:
  - 736
  - 1280
  # Random square cropping during training to save memory (always used for the video model)
  square_crop: true
  # Enable data augmentation during training
  data_augmentation: true
  # Indicate if the labels are a probabilty distribution instead of class labels. For knowledge distillation.
  soft_labels: false
  # Enables knowledge distillation for the dataset. See data.video.video_dataset.VideoLogitsDataset
  logit_distillation: false
  # Location of the logits from the teacher model for knowledge distillation. Should point to the train_logits folder of the teacher model´s results
  logits_folder: null
  # Min number of frames for a video to be counted. Should be left to 0 for all datasets.
  min_vid_len: 0
  # Number of labeled frames used for training per video. Should use train_skip_frames instead.
  labeled_frames_per_vid: null
  # Number of frames to skip between training samples. Should only be used for knowledge distillation or when the dataset is fully labeled.
  train_skip_frames: null
  # Number of frames to skip during validation step. Only useful for fully labeled datasets. Leave to 1 for others.
  val_skip_frames: 1
  # Cap validation samples to reduce memory during metric aggregation (DDP eval)
  val_max_samples: 2000
  # Method of computation of the homographies for the global registration.
  opencv_homos: false
  # Algorithm used for the opencv registration. Unused if opencv_homos is false.
  opencv_model_type: akaze

########################################################################################################################################
# Config for the base image model. It has to be an existing checkpoint with config file, even if untrained.
image_model_cfg:
  # Folder where the image model checkpoint is saved (should be the same for all image models).
  image_save_dir: /data1/wangcl/project/SSP/kitti360/image
  # Sub-folder of the previous folder if needed. Should include the final /
  checkpoint_folder: ""
  # Name of the checkpoint folder, in the format (optional@)date where the config file is date.yaml and the checkpoint date.pth.tar
  checkpoint_name: "28-12_21-56"
  # If loading the highest val mIoU image model checkpoint. Never used. Will load best_model_3_date.pth.tar.
  best_model: False

########################################################################################################################################
# Video model
video_model_cfg:
  # Model type, should always be "base".
  model_name: "base"
  # Consistency loss weight. Used values are 0.5 for normal training, 135000. for knowledge distillation.
  const_loss_lb: 0.5
  # For knowledge distillation. Post-processing of the teacher model´s logits to make them consistent with optical flow.
  kd_consistent_labels: true
  # For knowledge distillation. Weight of the segmentation loss on the past frame.
  last_out_loss_lb: 1.
  # Layer choice for the mixing of predictions. See models.models_consistency.parse_mixing_ops for the full list.
  pred_mixing_op: "id"
  # Layer choice for the similarity map computation. See models.models_consistency.parse_sim_ops for the full list.
  sim_op: "conv"
  # If the logits are upsampled to the full size before mixing.
  upsample_before_mixing: true
  # Interpolation method for the upsampling. Should be consistent with the image model config.
  upsampling: bicubic
  # Registration model for the homography computation on gpu. From the Kornia library.
  k_registrator_model: disk
  # Homography determinant epsilon for invertibility check
  homo_det_eps: 1.0e-8
  # Fallback behavior for invalid homographies: "identity" or "skip"
  homo_fallback: "identity"

########################################################################################################################################
training_cfg:
  # Total number of epochs, used for the learning rate scheduler.
  num_epochs: 250
  # Number of training epochs, can be lower than num_epochs to extand the learning rate scheduling beyond the training epochs.
  early_stopping: 200
  # Batch size
  batch_size: 9
  # num_workers in dataloader
  num_workers: 8
  # Learning rate
  lr: 5.0e-05
  # Train a layer with a different learning rate than the rest of the model, never used.
  trained_layer: null
  secondary_lr: 0.0001
  # Save checkpoints every N epochs
  save_every: 10
  # Always save latest checkpoint
  save_last: true

########################################################################################################################################
optim_cfg:
  # Optimizer, see utils.optim_utils.get_optimizer_scheduler
  optimizer: adamw
  # Learning rate scheduler, see utils.optim_utils.get_optimizer_scheduler
  scheduler: cosine
  # Weight decay used in optimizer
  weight_decay: 0.05
  # For SGD optimizer
  momentum: 0.9
  # Scheduler parameters
  warmup_epochs: 2
  # For polynomial scheduler
  scheduler_power: 0.9
  # Relative to the base lr
  start_lr: 0.0001
  final_lr: 0.0001

########################################################################################################################################
loss_cfg:
  # Segmentation loss function, see utils.optim_utils.get_criterion.
  loss_func: crossentropy
  # Class weigths in the segmentation loss, only for crossentropy.
  class_weights: null
  # For kldiv, if label need softmax applied.
  softmax_on_target: true
  # Temperature in kldiv for knowledge distillation. Only used 2.
  temperature: 2
